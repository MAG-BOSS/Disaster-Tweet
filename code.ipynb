{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:43.199190Z",
     "iopub.status.busy": "2021-01-01T11:54:43.198577Z",
     "iopub.status.idle": "2021-01-01T11:54:43.205958Z",
     "shell.execute_reply": "2021-01-01T11:54:43.206497Z"
    },
    "papermill": {
     "duration": 0.019728,
     "end_time": "2021-01-01T11:54:43.206697",
     "exception": false,
     "start_time": "2021-01-01T11:54:43.186969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:43.228309Z",
     "iopub.status.busy": "2021-01-01T11:54:43.227712Z",
     "iopub.status.idle": "2021-01-01T11:54:43.291530Z",
     "shell.execute_reply": "2021-01-01T11:54:43.291993Z"
    },
    "papermill": {
     "duration": 0.07465,
     "end_time": "2021-01-01T11:54:43.292127",
     "exception": false,
     "start_time": "2021-01-01T11:54:43.217477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:43.317601Z",
     "iopub.status.busy": "2021-01-01T11:54:43.316935Z",
     "iopub.status.idle": "2021-01-01T11:54:43.344182Z",
     "shell.execute_reply": "2021-01-01T11:54:43.343623Z"
    },
    "papermill": {
     "duration": 0.043055,
     "end_time": "2021-01-01T11:54:43.344286",
     "exception": false,
     "start_time": "2021-01-01T11:54:43.301231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:43.369906Z",
     "iopub.status.busy": "2021-01-01T11:54:43.369303Z",
     "iopub.status.idle": "2021-01-01T11:54:44.903439Z",
     "shell.execute_reply": "2021-01-01T11:54:44.902840Z"
    },
    "papermill": {
     "duration": 1.549906,
     "end_time": "2021-01-01T11:54:44.903561",
     "exception": false,
     "start_time": "2021-01-01T11:54:43.353655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2533</td>\n",
       "      <td>0.332720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>61</td>\n",
       "      <td>0.008013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total   Percent\n",
       "location   2533  0.332720\n",
       "keyword      61  0.008013\n",
       "target        0  0.000000\n",
       "text          0  0.000000\n",
       "id            0  0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#get total count of data including missing data\n",
    "total = data_train.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "#get percent of missing data relevant to all data\n",
    "percent = (data_train.isnull().sum()/data_train.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(data_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:44.930155Z",
     "iopub.status.busy": "2021-01-01T11:54:44.927986Z",
     "iopub.status.idle": "2021-01-01T11:54:44.933513Z",
     "shell.execute_reply": "2021-01-01T11:54:44.932950Z"
    },
    "papermill": {
     "duration": 0.019793,
     "end_time": "2021-01-01T11:54:44.933615",
     "exception": false,
     "start_time": "2021-01-01T11:54:44.913822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location and keyword columns droped successfully\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.drop(['location','keyword'], axis=1)\n",
    "print(\"location and keyword columns droped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:44.956952Z",
     "iopub.status.busy": "2021-01-01T11:54:44.956329Z",
     "iopub.status.idle": "2021-01-01T11:54:44.962257Z",
     "shell.execute_reply": "2021-01-01T11:54:44.962738Z"
    },
    "papermill": {
     "duration": 0.018957,
     "end_time": "2021-01-01T11:54:44.962848",
     "exception": false,
     "start_time": "2021-01-01T11:54:44.943891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id column droped successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_train = data_train.drop('id', axis=1)\n",
    "print(\"id column droped successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:44.987380Z",
     "iopub.status.busy": "2021-01-01T11:54:44.986759Z",
     "iopub.status.idle": "2021-01-01T11:54:44.996038Z",
     "shell.execute_reply": "2021-01-01T11:54:44.995595Z"
    },
    "papermill": {
     "duration": 0.022416,
     "end_time": "2021-01-01T11:54:44.996124",
     "exception": false,
     "start_time": "2021-01-01T11:54:44.973708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:54:45.028185Z",
     "iopub.status.busy": "2021-01-01T11:54:45.027576Z",
     "iopub.status.idle": "2021-01-01T11:55:05.736782Z",
     "shell.execute_reply": "2021-01-01T11:55:05.735928Z"
    },
    "papermill": {
     "duration": 20.729112,
     "end_time": "2021-01-01T11:55:05.736914",
     "exception": false,
     "start_time": "2021-01-01T11:54:45.007802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created successfully\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus  = []\n",
    "pstem = PorterStemmer()\n",
    "for i in range(data_train['text'].shape[0]):\n",
    "    #Remove unwanted words\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", ' ', data_train['text'][i])\n",
    "    #Transform words to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "    #Remove stopwords then Stemming it\n",
    "    tweet = [pstem.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    #Append cleaned tweet to corpus\n",
    "    corpus.append(tweet)\n",
    "    \n",
    "print(\"Corpus created successfully\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:55:05.794215Z",
     "iopub.status.busy": "2021-01-01T11:55:05.768341Z",
     "iopub.status.idle": "2021-01-01T11:55:05.835727Z",
     "shell.execute_reply": "2021-01-01T11:55:05.835123Z"
    },
    "papermill": {
     "duration": 0.086846,
     "end_time": "2021-01-01T11:55:05.835825",
     "exception": false,
     "start_time": "2021-01-01T11:55:05.748979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Frequent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>4746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>4721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dispers</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mre</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dchfpxgi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ughptzjulk</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rskq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18889 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word Frequent\n",
       "co                   4746\n",
       "http                 4721\n",
       "like                  411\n",
       "fire                  363\n",
       "amp                   344\n",
       "...                   ...\n",
       "dispers                 1\n",
       "mre                     1\n",
       "dchfpxgi                1\n",
       "ughptzjulk              1\n",
       "rskq                    1\n",
       "\n",
       "[18889 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create our dictionary \n",
    "uniqueWordFrequents = {}\n",
    "for tweet in corpus:\n",
    "    for word in tweet.split():\n",
    "        if(word in uniqueWordFrequents.keys()):\n",
    "            uniqueWordFrequents[word] += 1\n",
    "        else:\n",
    "            uniqueWordFrequents[word] = 1\n",
    "            \n",
    "#Convert dictionary to dataFrame\n",
    "\n",
    "uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
    "uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
    "uniqueWordFrequents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-01-01T11:55:05.872101Z",
     "iopub.status.busy": "2021-01-01T11:55:05.866016Z",
     "iopub.status.idle": "2021-01-01T11:55:05.886707Z",
     "shell.execute_reply": "2021-01-01T11:55:05.886228Z"
    },
    "papermill": {
     "duration": 0.038556,
     "end_time": "2021-01-01T11:55:05.886823",
     "exception": false,
     "start_time": "2021-01-01T11:55:05.848267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['co',\n",
       " 'http',\n",
       " 'like',\n",
       " 'fire',\n",
       " 'amp',\n",
       " 'get',\n",
       " 'bomb',\n",
       " 'new',\n",
       " 'via',\n",
       " 'u',\n",
       " 'news',\n",
       " 'go',\n",
       " 'one',\n",
       " 'peopl',\n",
       " 'time',\n",
       " 'kill',\n",
       " 'burn',\n",
       " 'year',\n",
       " 'w',\n",
       " 'video',\n",
       " 'flood',\n",
       " 'crash',\n",
       " 'emerg',\n",
       " 'disast',\n",
       " 'bodi',\n",
       " 'attack',\n",
       " 'build',\n",
       " 'day',\n",
       " 'p',\n",
       " 'look',\n",
       " 'say',\n",
       " 'fatal',\n",
       " 'home',\n",
       " 'x',\n",
       " 'love',\n",
       " 'polic',\n",
       " 'would',\n",
       " 'c',\n",
       " 'r',\n",
       " 'make',\n",
       " 'us',\n",
       " 'famili',\n",
       " 'evacu',\n",
       " 'b',\n",
       " 'still',\n",
       " 'storm',\n",
       " 'train',\n",
       " 'see',\n",
       " 'come',\n",
       " 'back',\n",
       " 'know',\n",
       " 'e',\n",
       " 'live',\n",
       " 'california',\n",
       " 'n',\n",
       " 'suicid',\n",
       " 'watch',\n",
       " 'bag',\n",
       " 'want',\n",
       " 'world',\n",
       " 'car',\n",
       " 'death',\n",
       " 'man',\n",
       " 'collaps',\n",
       " 'derail',\n",
       " 'scream',\n",
       " 'rt',\n",
       " 'got',\n",
       " 'pm',\n",
       " 'l',\n",
       " 'first',\n",
       " 'take',\n",
       " 'caus',\n",
       " 'let',\n",
       " 'v',\n",
       " 'think',\n",
       " 'two',\n",
       " 'nuclear',\n",
       " 'h',\n",
       " 'war',\n",
       " 'drown',\n",
       " 'today',\n",
       " 'need',\n",
       " 'g',\n",
       " 'work',\n",
       " 'wreck',\n",
       " 'dead',\n",
       " 'accid',\n",
       " 'k',\n",
       " 'youtub',\n",
       " 'deton',\n",
       " 'destroy',\n",
       " 'gt',\n",
       " 'hiroshima',\n",
       " 'q',\n",
       " 'hijack',\n",
       " 'plan',\n",
       " 'full',\n",
       " 'feel',\n",
       " 'old',\n",
       " 'life',\n",
       " 'obliter',\n",
       " 'fuck',\n",
       " 'good',\n",
       " 'help',\n",
       " 'fear',\n",
       " 'weapon',\n",
       " 'murder',\n",
       " 'way',\n",
       " 'may',\n",
       " 'j',\n",
       " 'surviv',\n",
       " 'injuri',\n",
       " 'last',\n",
       " 'wound',\n",
       " 'even',\n",
       " 'z',\n",
       " 'f',\n",
       " 'mani',\n",
       " 'could',\n",
       " 'devast',\n",
       " 'rescu',\n",
       " 'use',\n",
       " 'servic',\n",
       " 'die',\n",
       " 'wildfir',\n",
       " 'report',\n",
       " 'hazard',\n",
       " 'run',\n",
       " 'riot',\n",
       " 'explod',\n",
       " 'call',\n",
       " 'read',\n",
       " 'save',\n",
       " 'mass',\n",
       " 'wave',\n",
       " 'mh',\n",
       " 'collid',\n",
       " 'best',\n",
       " 'damag',\n",
       " 'right',\n",
       " 'hostag',\n",
       " 'thing',\n",
       " 'quarantin',\n",
       " 'pleas',\n",
       " 'anoth',\n",
       " 'catastroph',\n",
       " 'armi',\n",
       " 'school',\n",
       " 'realli',\n",
       " 'warn',\n",
       " 'crush',\n",
       " 'lol',\n",
       " 'hous',\n",
       " 'citi',\n",
       " 'water',\n",
       " 'fall',\n",
       " 'miss',\n",
       " 'black',\n",
       " 'photo',\n",
       " 'stop',\n",
       " 'thank',\n",
       " 'casualti',\n",
       " 'forest',\n",
       " 'hot',\n",
       " 'state',\n",
       " 'hit',\n",
       " 'god',\n",
       " 'delug',\n",
       " 'obama',\n",
       " 'hope',\n",
       " 'much',\n",
       " 'demolish',\n",
       " 'set',\n",
       " 'tri',\n",
       " 'northern',\n",
       " 'electrocut',\n",
       " 'reddit',\n",
       " 'confirm',\n",
       " 'cross',\n",
       " 'never',\n",
       " 'great',\n",
       " 'chang',\n",
       " 'end',\n",
       " 'flame',\n",
       " 'legionnair',\n",
       " 'bomber',\n",
       " 'head',\n",
       " 'play',\n",
       " 'break',\n",
       " 'investig',\n",
       " 'siren',\n",
       " 'bioterror',\n",
       " 'start',\n",
       " 'rain',\n",
       " 'releas',\n",
       " 'desol',\n",
       " 'latest',\n",
       " 'japan',\n",
       " 'im',\n",
       " 'th',\n",
       " 'typhoon',\n",
       " 'area',\n",
       " 'issu',\n",
       " 'injur',\n",
       " 'show',\n",
       " 'atom',\n",
       " 'everi',\n",
       " 'near',\n",
       " 'everyon',\n",
       " 'top',\n",
       " 'women',\n",
       " 'said',\n",
       " 'updat',\n",
       " 'terror',\n",
       " 'offici',\n",
       " 'content',\n",
       " 'thunderstorm',\n",
       " 'oil',\n",
       " 'happen',\n",
       " 'ever',\n",
       " 'blaze',\n",
       " 'annihil',\n",
       " 'sever',\n",
       " 'wind',\n",
       " 'shit',\n",
       " 'face',\n",
       " 'land',\n",
       " 'truck',\n",
       " 'sign',\n",
       " 'post',\n",
       " 'st',\n",
       " 'girl',\n",
       " 'game',\n",
       " 'sinc',\n",
       " 'smoke',\n",
       " 'natur',\n",
       " 'movi',\n",
       " 'check',\n",
       " 'offic',\n",
       " 'earthquak',\n",
       " 'charg',\n",
       " 'boy',\n",
       " 'night',\n",
       " 'found',\n",
       " 'pick',\n",
       " 'ass',\n",
       " 'militari',\n",
       " 'follow',\n",
       " 'weather',\n",
       " 'next',\n",
       " 'without',\n",
       " 'stori',\n",
       " 'well',\n",
       " 'respond',\n",
       " 'suspect',\n",
       " 'keep',\n",
       " 'declar',\n",
       " 'littl',\n",
       " 'wild',\n",
       " 'debri',\n",
       " 'malaysia',\n",
       " 'explos',\n",
       " 'sound',\n",
       " 'sink',\n",
       " 'structur',\n",
       " 'heat',\n",
       " 'guy',\n",
       " 'terrorist',\n",
       " 'food',\n",
       " 'danger',\n",
       " 'move',\n",
       " 'migrant',\n",
       " 'lightn',\n",
       " 'refuge',\n",
       " 'job',\n",
       " 'fan',\n",
       " 'fight',\n",
       " 'high',\n",
       " 'road',\n",
       " 'trap',\n",
       " 'hurrican',\n",
       " 'yr',\n",
       " 'alway',\n",
       " 'spill',\n",
       " 'made',\n",
       " 'week',\n",
       " 'battl',\n",
       " 'bad',\n",
       " 'blood',\n",
       " 'thunder',\n",
       " 'light',\n",
       " 'hail',\n",
       " 'hour',\n",
       " 'red',\n",
       " 'market',\n",
       " 'free',\n",
       " 'ruin',\n",
       " 'bloodi',\n",
       " 'nation',\n",
       " 'loud',\n",
       " 'summer',\n",
       " 'inund',\n",
       " 'also',\n",
       " 'leav',\n",
       " 'care',\n",
       " 'air',\n",
       " 'kid',\n",
       " 'someon',\n",
       " 'failur',\n",
       " 'lot',\n",
       " 'long',\n",
       " 'put',\n",
       " 'power',\n",
       " 'minut',\n",
       " 'gonna',\n",
       " 'china',\n",
       " 'busi',\n",
       " 'shot',\n",
       " 'tonight',\n",
       " 'outbreak',\n",
       " 'turn',\n",
       " 'destruct',\n",
       " 'bu',\n",
       " 'survivor',\n",
       " 'harm',\n",
       " 'big',\n",
       " 'sinkhol',\n",
       " 'collis',\n",
       " 'ok',\n",
       " 'panic',\n",
       " 'eye',\n",
       " 'ambul',\n",
       " 'saudi',\n",
       " 'displac',\n",
       " 'tornado',\n",
       " 'babi',\n",
       " 'bridg',\n",
       " 'person',\n",
       " 'affect',\n",
       " 'whole',\n",
       " 'order',\n",
       " 'wreckag',\n",
       " 'island',\n",
       " 'phone',\n",
       " 'rememb',\n",
       " 'drought',\n",
       " 'drive',\n",
       " 'total',\n",
       " 'landslid',\n",
       " 'islam',\n",
       " 'part',\n",
       " 'heart',\n",
       " 'real',\n",
       " 'around',\n",
       " 'windstorm',\n",
       " 'friend',\n",
       " 'close',\n",
       " 'wait',\n",
       " 'rescuer',\n",
       " 'send',\n",
       " 'hundr',\n",
       " 'twister',\n",
       " 'trauma',\n",
       " 'word',\n",
       " 'counti',\n",
       " 'dust',\n",
       " 'white',\n",
       " 'ship',\n",
       " 'chemic',\n",
       " 'self',\n",
       " 'open',\n",
       " 'sunk',\n",
       " 'group',\n",
       " 'airplan',\n",
       " 'boat',\n",
       " 'effect',\n",
       " 'away',\n",
       " 'possibl',\n",
       " 'cliff',\n",
       " 'plane',\n",
       " 'sandstorm',\n",
       " 'bang',\n",
       " 'august',\n",
       " 'stock',\n",
       " 'whirlwind',\n",
       " 'curfew',\n",
       " 'bleed',\n",
       " 'massacr',\n",
       " 'engulf',\n",
       " 'violent',\n",
       " 'mudslid',\n",
       " 'woman',\n",
       " 'twitter',\n",
       " 'armageddon',\n",
       " 'half',\n",
       " 'famin',\n",
       " 'thought',\n",
       " 'pic',\n",
       " 'saw',\n",
       " 'wanna',\n",
       " 'song',\n",
       " 'iran',\n",
       " 'unit',\n",
       " 'tragedi',\n",
       " 'oh',\n",
       " 'apocalyps',\n",
       " 'intern',\n",
       " 'second',\n",
       " 'raze',\n",
       " 'memori',\n",
       " 'search',\n",
       " 'mosqu',\n",
       " 'troubl',\n",
       " 'deal',\n",
       " 'better',\n",
       " 'blast',\n",
       " 'rise',\n",
       " 'came',\n",
       " 'traumatis',\n",
       " 'link',\n",
       " 'least',\n",
       " 'past',\n",
       " 'fedex',\n",
       " 'stay',\n",
       " 'left',\n",
       " 'tomorrow',\n",
       " 'blown',\n",
       " 'horribl',\n",
       " 'tell',\n",
       " 'secur',\n",
       " 'hear',\n",
       " 'insur',\n",
       " 'volcano',\n",
       " 'tsunami',\n",
       " 'sure',\n",
       " 'heard',\n",
       " 'anniversari',\n",
       " 'rd',\n",
       " 'zone',\n",
       " 'place',\n",
       " 'flatten',\n",
       " 'stand',\n",
       " 'talk',\n",
       " 'meltdown',\n",
       " 'must',\n",
       " 'pandemonium',\n",
       " 'went',\n",
       " 'ur',\n",
       " 'ban',\n",
       " 'peac',\n",
       " 'river',\n",
       " 'panick',\n",
       " 'book',\n",
       " 'govern',\n",
       " 'cool',\n",
       " 'isi',\n",
       " 'blew',\n",
       " 'beauti',\n",
       " 'lava',\n",
       " 'swallow',\n",
       " 'men',\n",
       " 'demolit',\n",
       " 'reunion',\n",
       " 'calgari',\n",
       " 'soon',\n",
       " 'project',\n",
       " 'expect',\n",
       " 'support',\n",
       " 'ladi',\n",
       " 'shoot',\n",
       " 'someth',\n",
       " 'final',\n",
       " 'actual',\n",
       " 'give',\n",
       " 'ebay',\n",
       " 'star',\n",
       " 'hellfir',\n",
       " 'shoulder',\n",
       " 'south',\n",
       " 'street',\n",
       " 'eyewit',\n",
       " 'abc',\n",
       " 'human',\n",
       " 'cyclon',\n",
       " 'dog',\n",
       " 'rainstorm',\n",
       " 'case',\n",
       " 'find',\n",
       " 'mean',\n",
       " 'believ',\n",
       " 'mp',\n",
       " 'india',\n",
       " 'longer',\n",
       " 'km',\n",
       " 'arson',\n",
       " 'three',\n",
       " 'airport',\n",
       " 'lt',\n",
       " 'american',\n",
       " 'music',\n",
       " 'walk',\n",
       " 'traffic',\n",
       " 'goe',\n",
       " 'reason',\n",
       " 'due',\n",
       " 'america',\n",
       " 'pkk',\n",
       " 'share',\n",
       " 'countri',\n",
       " 'yet',\n",
       " 'name',\n",
       " 'noth',\n",
       " 'ye',\n",
       " 'lab',\n",
       " 'wake',\n",
       " 'line',\n",
       " 'site',\n",
       " 'side',\n",
       " 'level',\n",
       " 'gun',\n",
       " 'bush',\n",
       " 'listen',\n",
       " 'offens',\n",
       " 'prebreak',\n",
       " 'media',\n",
       " 'control',\n",
       " 'health',\n",
       " 'fun',\n",
       " 'bc',\n",
       " 'avalanch',\n",
       " 'nigga',\n",
       " 'win',\n",
       " 'hate',\n",
       " 'stretcher',\n",
       " 'team',\n",
       " 'forc',\n",
       " 'horror',\n",
       " 'block',\n",
       " 'insid',\n",
       " 'wonder',\n",
       " 'learn',\n",
       " 'alreadi',\n",
       " 'done',\n",
       " 'polici',\n",
       " 'upheav',\n",
       " 'bring',\n",
       " 'ablaz',\n",
       " 'gener',\n",
       " 'ad',\n",
       " 'reactor',\n",
       " 'seismic',\n",
       " 'blight',\n",
       " 'action',\n",
       " 'sue',\n",
       " 'imag',\n",
       " 'snowstorm',\n",
       " 'hold',\n",
       " 'crew',\n",
       " 'turkey',\n",
       " 'isra',\n",
       " 'bar',\n",
       " 'lead',\n",
       " 'transport',\n",
       " 'ask',\n",
       " 'la',\n",
       " 'far',\n",
       " 'becom',\n",
       " 'tweet',\n",
       " 'point',\n",
       " 'rule',\n",
       " 'vehicl',\n",
       " 'aug',\n",
       " 'ago',\n",
       " 'mark',\n",
       " 'month',\n",
       " 'nearbi',\n",
       " 'comput',\n",
       " 'north',\n",
       " 'nowplay',\n",
       " 'helicopt',\n",
       " 'victim',\n",
       " 'happi',\n",
       " 'lie',\n",
       " 'yeah',\n",
       " 'brown',\n",
       " 'low',\n",
       " 'hand',\n",
       " 'special',\n",
       " 'villag',\n",
       " 'major',\n",
       " 'soudelor',\n",
       " 'rubbl',\n",
       " 'cover',\n",
       " 'children',\n",
       " 'firefight',\n",
       " 'west',\n",
       " 'histori',\n",
       " 'conclus',\n",
       " 'tree',\n",
       " 'center',\n",
       " 'tv',\n",
       " 'anyth',\n",
       " 'saipan',\n",
       " 'utc',\n",
       " 'knock',\n",
       " 'begin',\n",
       " 'bigger',\n",
       " 'flag',\n",
       " 'outsid',\n",
       " 'alarm',\n",
       " 'mayb',\n",
       " 'morn',\n",
       " 'pakistan',\n",
       " 'hell',\n",
       " 'handbag',\n",
       " 'data',\n",
       " 'hat',\n",
       " 'rock',\n",
       " 'spot',\n",
       " 'aircraft',\n",
       " 'lost',\n",
       " 'almost',\n",
       " 'hey',\n",
       " 'bestnaijamad',\n",
       " 'strike',\n",
       " 'pay',\n",
       " 'blizzard',\n",
       " 'camp',\n",
       " 'ca',\n",
       " 'act',\n",
       " 'probabl',\n",
       " 'liter',\n",
       " 'moment',\n",
       " 'number',\n",
       " 'onlin',\n",
       " 'sit',\n",
       " 'prepar',\n",
       " 'mom',\n",
       " 'reuter',\n",
       " 'secret',\n",
       " 'money',\n",
       " 'uk',\n",
       " 'allow',\n",
       " 'christian',\n",
       " 'fast',\n",
       " 'million',\n",
       " 'claim',\n",
       " 'miner',\n",
       " 'muslim',\n",
       " 'cake',\n",
       " 'continu',\n",
       " 'anyon',\n",
       " 'coach',\n",
       " 'cours',\n",
       " 'trench',\n",
       " 'pass',\n",
       " 'drink',\n",
       " 'though',\n",
       " 'pretti',\n",
       " 'track',\n",
       " 'space',\n",
       " 'hailstorm',\n",
       " 'public',\n",
       " 'ball',\n",
       " 'damn',\n",
       " 'interest',\n",
       " 'properti',\n",
       " 'amid',\n",
       " 'thousand',\n",
       " 'park',\n",
       " 'might',\n",
       " 'mountain',\n",
       " 'manslaught',\n",
       " 'town',\n",
       " 'aftershock',\n",
       " 'coupl',\n",
       " 'film',\n",
       " 'activ',\n",
       " 'everyth',\n",
       " 'hollywood',\n",
       " 'arsonist',\n",
       " 'nearli',\n",
       " 'chanc',\n",
       " 'crisi',\n",
       " 'class',\n",
       " 'complet',\n",
       " 'seen',\n",
       " 'respons',\n",
       " 'sensor',\n",
       " 'expert',\n",
       " 'okay',\n",
       " 'seek',\n",
       " 'caught',\n",
       " 'problem',\n",
       " 'sky',\n",
       " 'blue',\n",
       " 'date',\n",
       " 'costlier',\n",
       " 'worri',\n",
       " 'usa',\n",
       " 'sorri',\n",
       " 'australia',\n",
       " 'wow',\n",
       " 'emot',\n",
       " 'child',\n",
       " 'favorit',\n",
       " 'wrong',\n",
       " 'em',\n",
       " 'ignit',\n",
       " 'ga',\n",
       " 'refugio',\n",
       " 'giant',\n",
       " 'youth',\n",
       " 'leather',\n",
       " 'flash',\n",
       " 'gbbo',\n",
       " 'lord',\n",
       " 'cop',\n",
       " 'texa',\n",
       " 'omg',\n",
       " 'east',\n",
       " 'mayhem',\n",
       " 'huge',\n",
       " 'trust',\n",
       " 'russian',\n",
       " 'crazi',\n",
       " 'fukushima',\n",
       " 'idea',\n",
       " 'mount',\n",
       " 'cri',\n",
       " 'result',\n",
       " 'pain',\n",
       " 'other',\n",
       " 'escap',\n",
       " 'entir',\n",
       " 'avoid',\n",
       " 'bbc',\n",
       " 'daili',\n",
       " 'nd',\n",
       " 'scare',\n",
       " 'heavi',\n",
       " 'passeng',\n",
       " 'worst',\n",
       " 'comment',\n",
       " 'angri',\n",
       " 'nw',\n",
       " 'meet',\n",
       " 'compani',\n",
       " 'mine',\n",
       " 'view',\n",
       " 'vs',\n",
       " 'appear',\n",
       " 'across',\n",
       " 'creat',\n",
       " 'dr',\n",
       " 'art',\n",
       " 'dont',\n",
       " 'extrem',\n",
       " 'mph',\n",
       " 'grow',\n",
       " 'chicago',\n",
       " 'anthrax',\n",
       " 'haha',\n",
       " 'mishap',\n",
       " 'meek',\n",
       " 'drake',\n",
       " 'hard',\n",
       " 'toddler',\n",
       " 'record',\n",
       " 'poor',\n",
       " 'test',\n",
       " 'rate',\n",
       " 'polit',\n",
       " 'agre',\n",
       " 'beach',\n",
       " 'wed',\n",
       " 'pilot',\n",
       " 'radio',\n",
       " 'truth',\n",
       " 'milit',\n",
       " 'sea',\n",
       " 'cnn',\n",
       " 'gem',\n",
       " 'captur',\n",
       " 'arriv',\n",
       " 'carri']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
    "print(uniqueWordFrequents.shape)\n",
    "#uniqueWordFrequents\n",
    "words=[]\n",
    "for word in uniqueWordFrequents.index:\n",
    "    words.append(word)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:55:05.938675Z",
     "iopub.status.busy": "2021-01-01T11:55:05.928351Z",
     "iopub.status.idle": "2021-01-01T11:55:06.123295Z",
     "shell.execute_reply": "2021-01-01T11:55:06.122740Z"
    },
    "papermill": {
     "duration": 0.223442,
     "end_time": "2021-01-01T11:55:06.123453",
     "exception": false,
     "start_time": "2021-01-01T11:55:05.900011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
    "bagOfWords = counVec.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-01T11:55:06.162065Z",
     "iopub.status.busy": "2021-01-01T11:55:06.161269Z",
     "iopub.status.idle": "2021-01-01T11:55:19.400135Z",
     "shell.execute_reply": "2021-01-01T11:55:19.399603Z"
    },
    "papermill": {
     "duration": 13.262902,
     "end_time": "2021-01-01T11:55:19.400267",
     "exception": false,
     "start_time": "2021-01-01T11:55:06.137365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = bagOfWords\n",
    "y = data_train['target']\n",
    "decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = None, \n",
    "                                           splitter='best', \n",
    "                                           random_state=55)\n",
    "\n",
    "decisionTreeModel.fit(X,y)\n",
    "print(\"model trained\")\n",
    "queries=[]\n",
    "for tweet in test_data['text']:\n",
    "    pstem = PorterStemmer()\n",
    "    text = tweet\n",
    "    text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    query = []\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            query.append(1)\n",
    "        else:\n",
    "            query.append(0)\n",
    "    queries.append(query)\n",
    "predictions = (decisionTreeModel.predict(queries))\n",
    "    \n",
    "output = pd.DataFrame({'id': test_data.id, 'target': predictions})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 40.460167,
   "end_time": "2021-01-01T11:55:19.523344",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-01T11:54:39.063177",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
